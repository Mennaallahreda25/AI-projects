{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f384aab",
   "metadata": {},
   "source": [
    "# translation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4de5128",
   "metadata": {},
   "source": [
    "# first way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f76d010f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-02T04:20:47.180013Z",
     "start_time": "2022-12-02T04:20:29.569809Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textblob in c:\\users\\hp\\anaconda3\\lib\\site-packages (0.17.1)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -pencv-python-headless (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python-headless (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python-headless (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python-headless (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python-headless (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python-headless (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: nltk>=3.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from textblob) (3.7)\n",
      "Requirement already satisfied: tqdm in c:\\users\\hp\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (4.64.0)\n",
      "Requirement already satisfied: joblib in c:\\users\\hp\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (1.1.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (2022.3.15)\n",
      "Requirement already satisfied: click in c:\\users\\hp\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (8.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\anaconda3\\lib\\site-packages (from click->nltk>=3.1->textblob) (0.4.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e185555a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-02T04:28:06.176135Z",
     "start_time": "2022-12-02T04:28:02.973364Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -pencv-python-headless (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python-headless (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "ERROR: Could not find a version that satisfies the requirement textblob.download_corpora (from versions: none)\n",
      "ERROR: No matching distribution found for textblob.download_corpora\n",
      "WARNING: Ignoring invalid distribution -pencv-python-headless (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python-headless (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python-headless (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install textblob.download_corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7488d72f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-02T04:28:43.418945Z",
     "start_time": "2022-12-02T04:28:43.400995Z"
    }
   },
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2c1650",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-12-02T04:55:21.814Z"
    }
   },
   "outputs": [],
   "source": [
    "text_blob_object= TextBlob(\"my name us menna . i live in Egypt\")\n",
    "print(text_blob_object.translate(to=\"ar\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f999fd",
   "metadata": {},
   "source": [
    "# second way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3e256698",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-02T04:40:00.155938Z",
     "start_time": "2022-12-02T04:39:32.354760Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting googletrans"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -pencv-python-headless (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python-headless (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python-headless (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -pencv-python-headless (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -pencv-python-headless (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -pencv-python-headless (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python-headless (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python-headless (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python-headless (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python-headless (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python-headless (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python-headless (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python-headless (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python-headless (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python-headless (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python-headless (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python-headless (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "spyder 5.1.5 requires pyqt5<5.13, which is not installed.\n",
      "spyder 5.1.5 requires pyqtwebengine<5.13, which is not installed.\n",
      "conda-repo-cli 1.0.4 requires pathlib, which is not installed.\n",
      "anaconda-project 0.10.2 requires ruamel-yaml, which is not installed.\n",
      "google-api-core 2.10.0 requires protobuf<5.0.0dev,>=3.20.1, but you have protobuf 3.19.6 which is incompatible.\n",
      "WARNING: Ignoring invalid distribution -pencv-python-headless (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python-headless (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python-headless (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Downloading googletrans-3.0.0.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting httpx==0.13.3\n",
      "  Downloading httpx-0.13.3-py3-none-any.whl (55 kB)\n",
      "     -------------------------------------- 55.1/55.1 kB 407.8 kB/s eta 0:00:00\n",
      "Requirement already satisfied: sniffio in c:\\users\\hp\\anaconda3\\lib\\site-packages (from httpx==0.13.3->googletrans) (1.2.0)\n",
      "Collecting idna==2.*\n",
      "  Downloading idna-2.10-py2.py3-none-any.whl (58 kB)\n",
      "     -------------------------------------- 58.8/58.8 kB 516.7 kB/s eta 0:00:00\n",
      "Collecting chardet==3.*\n",
      "  Downloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
      "     ------------------------------------ 133.4/133.4 kB 654.9 kB/s eta 0:00:00\n",
      "Requirement already satisfied: certifi in c:\\users\\hp\\anaconda3\\lib\\site-packages (from httpx==0.13.3->googletrans) (2021.10.8)\n",
      "Collecting hstspreload\n",
      "  Downloading hstspreload-2022.12.1-py3-none-any.whl (1.5 MB)\n",
      "     ---------------------------------------- 1.5/1.5 MB 812.9 kB/s eta 0:00:00\n",
      "Collecting rfc3986<2,>=1.3\n",
      "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
      "Collecting httpcore==0.9.*\n",
      "  Downloading httpcore-0.9.1-py3-none-any.whl (42 kB)\n",
      "     -------------------------------------- 42.6/42.6 kB 206.4 kB/s eta 0:00:00\n",
      "Collecting h2==3.*\n",
      "  Downloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\n",
      "     -------------------------------------- 65.0/65.0 kB 583.0 kB/s eta 0:00:00\n",
      "Collecting h11<0.10,>=0.8\n",
      "  Downloading h11-0.9.0-py2.py3-none-any.whl (53 kB)\n",
      "     -------------------------------------- 53.6/53.6 kB 466.3 kB/s eta 0:00:00\n",
      "Collecting hpack<4,>=3.0\n",
      "  Downloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\n",
      "Collecting hyperframe<6,>=5.2.0\n",
      "  Downloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\n",
      "Building wheels for collected packages: googletrans\n",
      "  Building wheel for googletrans (setup.py): started\n",
      "  Building wheel for googletrans (setup.py): finished with status 'done'\n",
      "  Created wheel for googletrans: filename=googletrans-3.0.0-py3-none-any.whl size=15749 sha256=9f7af951fc1e5ca00b466cea0891403bb7a83586c6c72778f661f326e888141b\n",
      "  Stored in directory: c:\\users\\hp\\appdata\\local\\pip\\cache\\wheels\\a7\\e7\\13\\e7e0741b248e1a1350da119d19e48e02a7b862edb936fb472a\n",
      "Successfully built googletrans\n",
      "Installing collected packages: rfc3986, hyperframe, hpack, h11, chardet, idna, hstspreload, h2, httpcore, httpx, googletrans\n",
      "  Attempting uninstall: h11\n",
      "    Found existing installation: h11 0.14.0\n",
      "    Uninstalling h11-0.14.0:\n",
      "      Successfully uninstalled h11-0.14.0\n",
      "  Attempting uninstall: chardet\n",
      "    Found existing installation: chardet 4.0.0\n",
      "    Uninstalling chardet-4.0.0:\n",
      "      Successfully uninstalled chardet-4.0.0\n",
      "  Attempting uninstall: idna\n",
      "    Found existing installation: idna 3.3\n",
      "    Uninstalling idna-3.3:\n",
      "      Successfully uninstalled idna-3.3\n",
      "Successfully installed chardet-3.0.4 googletrans-3.0.0 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2022.12.1 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 idna-2.10 rfc3986-1.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install googletrans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6b2bb615",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-02T04:40:22.942111Z",
     "start_time": "2022-12-02T04:40:22.648287Z"
    }
   },
   "outputs": [],
   "source": [
    "from googletrans import Translator, constants\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b2dd1323",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-02T04:40:34.983561Z",
     "start_time": "2022-12-02T04:40:34.948646Z"
    }
   },
   "outputs": [],
   "source": [
    "# init the Google API translator\n",
    "translator = Translator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2da025fd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-02T04:40:54.732695Z",
     "start_time": "2022-12-02T04:40:53.983941Z"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'group'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [16]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# translate a spanish text to english text (by default)\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m translation \u001b[38;5;241m=\u001b[39m \u001b[43mtranslator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranslate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHola Mundo\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtranslation\u001b[38;5;241m.\u001b[39morigin\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtranslation\u001b[38;5;241m.\u001b[39msrc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) --> \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtranslation\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtranslation\u001b[38;5;241m.\u001b[39mdest\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\googletrans\\client.py:182\u001b[0m, in \u001b[0;36mTranslator.translate\u001b[1;34m(self, text, dest, src, **kwargs)\u001b[0m\n\u001b[0;32m    179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[0;32m    181\u001b[0m origin \u001b[38;5;241m=\u001b[39m text\n\u001b[1;32m--> 182\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_translate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;66;03m# this code will be updated when the format is changed.\u001b[39;00m\n\u001b[0;32m    185\u001b[0m translated \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([d[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m d[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m data[\u001b[38;5;241m0\u001b[39m]])\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\googletrans\\client.py:78\u001b[0m, in \u001b[0;36mTranslator._translate\u001b[1;34m(self, text, dest, src, override)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_translate\u001b[39m(\u001b[38;5;28mself\u001b[39m, text, dest, src, override):\n\u001b[1;32m---> 78\u001b[0m     token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoken_acquirer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     79\u001b[0m     params \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mbuild_params(query\u001b[38;5;241m=\u001b[39mtext, src\u001b[38;5;241m=\u001b[39msrc, dest\u001b[38;5;241m=\u001b[39mdest,\n\u001b[0;32m     80\u001b[0m                                 token\u001b[38;5;241m=\u001b[39mtoken, override\u001b[38;5;241m=\u001b[39moverride)\n\u001b[0;32m     82\u001b[0m     url \u001b[38;5;241m=\u001b[39m urls\u001b[38;5;241m.\u001b[39mTRANSLATE\u001b[38;5;241m.\u001b[39mformat(host\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pick_service_url())\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\googletrans\\gtoken.py:194\u001b[0m, in \u001b[0;36mTokenAcquirer.do\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    193\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdo\u001b[39m(\u001b[38;5;28mself\u001b[39m, text):\n\u001b[1;32m--> 194\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    195\u001b[0m     tk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39macquire(text)\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tk\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\googletrans\\gtoken.py:62\u001b[0m, in \u001b[0;36mTokenAcquirer._update\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;66;03m# this will be the same as python code after stripping out a reserved word 'var'\u001b[39;00m\n\u001b[1;32m---> 62\u001b[0m code \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mRE_TKK\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroup\u001b[49m(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvar \u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# unescape special ascii characters such like a \\x3d(=)\u001b[39;00m\n\u001b[0;32m     64\u001b[0m code \u001b[38;5;241m=\u001b[39m code\u001b[38;5;241m.\u001b[39mencode()\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124municode-escape\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'group'"
     ]
    }
   ],
   "source": [
    "# translate a spanish text to english text (by default)\n",
    "translation = translator.translate(\"Hola Mundo\")\n",
    "print(f\"{translation.origin} ({translation.src}) --> {translation.text} ({translation.dest})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd680643",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-12-02T04:42:18.276Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip uninstall googletrans\n",
    "!pip install googletrans==3.1.0a0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8457a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# translate a spanish text to arabic for instance\n",
    "translation = translator.translate(\"Hola Mundo\", dest=\"ar\")\n",
    "print(f\"{translation.origin} ({translation.src}) --> {translation.text} ({translation.dest})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a56638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify source language\n",
    "translation = translator.translate(\"Wie gehts ?\", src=\"de\")\n",
    "print(f\"{translation.origin} ({translation.src}) --> {translation.text} ({translation.dest})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e30a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print all translations and other data\n",
    "pprint(translation.extra_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9ea099",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Translating List of Phrases\n",
    "#You can also pass a list of text to translate each sentence individually:\n",
    "\n",
    "# translate more than a phrase\n",
    "sentences = [\n",
    "    \"Hello everyone\",\n",
    "    \"How are you ?\",\n",
    "    \"Do you speak english ?\",\n",
    "    \"Good bye!\"\n",
    "]\n",
    "translations = translator.translate(sentences, dest=\"tr\")\n",
    "for translation in translations:\n",
    "    print(f\"{translation.origin} ({translation.src}) --> {translation.text} ({translation.dest})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8228d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# detect a language\n",
    "detection = translator.detect(\"नमस्ते दुनिया\")\n",
    "print(\"Language code:\", detection.lang)\n",
    "print(\"Confidence:\", detection.confidence)\n",
    "print(\"Language:\", constants.LANGUAGES[detection.lang])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41444278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print all available languages\n",
    "print(\"Total supported languages:\", len(constants.LANGUAGES))\n",
    "print(\"Languages:\")\n",
    "pprint(constants.LANGUAGES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e56a5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "source :\n",
    "    https://www.thepythoncode.com/article/translate-text-in-python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9287e76b",
   "metadata": {},
   "source": [
    "# summarize text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47799565",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "from heapq import nlargest\n",
    "text = \"Enter Text to Summarize\"\n",
    "if text.count(\". \") > 20:\n",
    "    length = int(round(text.count(\". \")/10, 0))\n",
    "else:\n",
    "    length = 1\n",
    "\n",
    "nopuch =[char for char in text if char not in string.punctuation]\n",
    "nopuch = \"\".join(nopuch)\n",
    "\n",
    "processed_text = [word for word in nopuch.split() if word.lower() not in nltk.corpus.stopwords.words('english')]\n",
    "\n",
    "word_freq = {}\n",
    "for word in processed_text:\n",
    "    if word not in word_freq:\n",
    "        word_freq[word] = 1\n",
    "    else:\n",
    "        word_freq[word] = word_freq[word] + 1\n",
    "\n",
    "max_freq = max(word_freq.values())\n",
    "for word in word_freq.keys():\n",
    "    word_freq[word] = (word_freq[word]/max_freq)\n",
    "\n",
    "sent_list = nltk.sent_tokenize(text)\n",
    "sent_score = {}\n",
    "for sent in sent_list:\n",
    "    for word in nltk.word_tokenize(sent.lower()):\n",
    "        if word in word_freq.keys():\n",
    "            if sent not in sent_score.keys():\n",
    "                sent_score[sent] = word_freq[word]\n",
    "            else:\n",
    "                sent_score[sent] = sent_score[sent] + word_freq[word]\n",
    "\n",
    "summary_sents = nlargest(length, sent_score, key=sent_score.get)\n",
    "summary = \" \".join(summary_sents)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2d0c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "source :\n",
    "    https://thecleverprogrammer.com/2020/12/31/text-summarization-with-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759b213b",
   "metadata": {},
   "source": [
    "# from video to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff58129",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ibm_watson\n",
    "!brew install ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8660b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "from ibm_watson import SpeechToTextV1\n",
    "from ibm_watson.websocket import RecognizeCallback, AudioSource\n",
    "from ibm_cloud_sdk_core.authenticators import IAMAuthenticator\n",
    "command = 'ffmpeg -i aiml.mkv -ab 160k -ar 44100 -vn audio.wav'\n",
    "subprocess.call(command, shell=True)\n",
    "apikey = ''\n",
    "url = ''\n",
    "# Setup service\n",
    "authenticator = IAMAuthenticator(apikey)\n",
    "stt = SpeechToTextV1(authenticator=authenticator)\n",
    "stt.set_service_url(url)\n",
    "with open('audio.wav', 'rb') as f:\n",
    "    res = stt.recognize(audio=f, content_type='audio/wav', model='en-AU_NarrowbandModel', continuous=True).get_result()\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad59a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Process Results and Output to Text\n",
    "len(res['results'])\n",
    "text = [result['alternatives'][0]['transcript'].rstrip() + '.\\n' for result in res['results']]\n",
    "text = [para[0].title() + para[1:] for para in text]\n",
    "transcript = ''.join(text)\n",
    "with open('output.txt', 'w') as out:\n",
    "    out.writelines(transcript)\n",
    "transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4d1519",
   "metadata": {},
   "outputs": [],
   "source": [
    "source: \n",
    "    https://github.com/nicknochnack/VideoToText/blob/master/Video%20to%20Text.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffdb5b8",
   "metadata": {},
   "source": [
    "# similarity sentance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47758d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "  \n",
    "X = input(\"Enter first string: \").lower() \n",
    "Y = input(\"Enter second string: \").lower() \n",
    "   \n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "  \n",
    "X = input(\"Enter first string: \").lower() \n",
    "Y = input(\"Enter second string: \").lower() \n",
    "   \n",
    "X_list = word_tokenize(X)  \n",
    "Y_list = word_tokenize(Y) \n",
    "  \n",
    "sw = stopwords.words('english')  \n",
    "l1 =[];l2 =[] \n",
    "   \n",
    "X_set = {w for w in X_list if not w in sw}  \n",
    "Y_set = {w for w in Y_list if not w in sw} \n",
    "    \n",
    "rvector = X_set.union(Y_set)  \n",
    "for w in rvector: \n",
    "    if w in X_set: l1.append(1)\n",
    "    else: l1.append(0) \n",
    "    if w in Y_set: l2.append(1) \n",
    "    else: l2.append(0) \n",
    "c = 0\n",
    "    \n",
    "for i in range(len(rvector)): \n",
    "        c+= l1[i]*l2[i] \n",
    "cosine = c / float((sum(l1)*sum(l2))**0.5) \n",
    "print(\"similarity: \", cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7601111a",
   "metadata": {},
   "outputs": [],
   "source": [
    "source: \n",
    "    https://www.codespeedy.com/find-sentence-similarity-in-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6882558b",
   "metadata": {},
   "source": [
    "# Paraphrase Text using Transformers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c437aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers sentencepiece sacremoses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f46543",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807ebac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PegasusForConditionalGeneration.from_pretrained(\"tuner007/pegasus_paraphrase\")\n",
    "tokenizer = PegasusTokenizerFast.from_pretrained(\"tuner007/pegasus_paraphrase\")\n",
    "def get_paraphrased_sentences(model, tokenizer, sentence, num_return_sequences=5, num_beams=5):\n",
    "  # tokenize the text to be form of a list of token IDs\n",
    "  inputs = tokenizer([sentence], truncation=True, padding=\"longest\", return_tensors=\"pt\")\n",
    "  # generate the paraphrased sentences\n",
    "  outputs = model.generate(\n",
    "    **inputs,\n",
    "    num_beams=num_beams,\n",
    "    num_return_sequences=num_return_sequences,\n",
    "  )\n",
    "  # decode the generated sentences using the tokenizer to get them back to text\n",
    "  return tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "sentence = \"Learning is the process of acquiring new understanding, knowledge, behaviors, skills, values, attitudes, and preferences.\"\n",
    "get_paraphrased_sentences(model, tokenizer, sentence, num_beams=10, num_return_sequences=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154fe486",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This section will explore the T5 architecture model that was fine-tuned on the PAWS dataset. PAWS consists of 108,463 human-labeled and 656k noisily labeled pairs. Let's load the model and the tokenizer:\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Vamsi/T5_Paraphrase_Paws\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"Vamsi/T5_Paraphrase_Paws\")\n",
    "#Let's use our previously defined function:\n",
    "get_paraphrased_sentences(model, tokenizer, \"One of the best ways to learn is to teach what you've already learned\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976a2fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parrot Paraphraser\n",
    "#Finally, let's use a fine-tuned T5 model architecture called Parrot. It is an augmentation framework built to speed-up training NLU models. The author of the fine-tuned model did a small library to perform paraphrasing. Let's install it:\n",
    "!pip install git+https://github.com/PrithivirajDamodaran/Parrot_Paraphraser.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648d836e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from parrot import Parrot\n",
    "\n",
    "parrot = Parrot()\n",
    "phrases = [\n",
    "  sentence,\n",
    "  \"One of the best ways to learn is to teach what you've already learned\",\n",
    "  \"Paraphrasing is the process of coming up with someone else's ideas in your own words\"\n",
    "]\n",
    "\n",
    "for phrase in phrases:\n",
    "  print(\"-\"*100)\n",
    "  print(\"Input_phrase: \", phrase)\n",
    "  print(\"-\"*100)\n",
    "  paraphrases = parrot.augment(input_phrase=phrase)\n",
    "  if paraphrases:\n",
    "    for paraphrase in paraphrases:\n",
    "      print(paraphrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c7ea00",
   "metadata": {},
   "outputs": [],
   "source": [
    "source:\n",
    "    https://www.thepythoncode.com/article/paraphrase-text-using-transformers-in-python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0145b014",
   "metadata": {},
   "source": [
    "# Named Entity Recognition (NER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8383b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "uploaded = files.upload()\n",
    "import pandas as pd\n",
    "data = pd.read_csv('ner_dataset.csv', encoding= 'unicode_escape')\n",
    "data.head()\n",
    "from itertools import chain\n",
    "def get_dict_map(data, token_or_tag):\n",
    "    tok2idx = {}\n",
    "    idx2tok = {}\n",
    "    \n",
    "    if token_or_tag == 'token':\n",
    "        vocab = list(set(data['Word'].to_list()))\n",
    "    else:\n",
    "        vocab = list(set(data['Tag'].to_list()))\n",
    "    \n",
    "    idx2tok = {idx:tok for  idx, tok in enumerate(vocab)}\n",
    "    tok2idx = {tok:idx for  idx, tok in enumerate(vocab)}\n",
    "    return tok2idx, idx2tok\n",
    "token2idx, idx2token = get_dict_map(data, 'token')\n",
    "tag2idx, idx2tag = get_dict_map(data, 'tag')\n",
    "data['Word_idx'] = data['Word'].map(token2idx)\n",
    "data['Tag_idx'] = data['Tag'].map(tag2idx)\n",
    "data_fillna = data.fillna(method='ffill', axis=0)\n",
    "# Groupby and collect columns\n",
    "data_group = data_fillna.groupby(\n",
    "['Sentence #'],as_index=False\n",
    ")['Word', 'POS', 'Tag', 'Word_idx', 'Tag_idx'].agg(lambda x: list(x))\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "def get_pad_train_test_val(data_group, data):\n",
    "\n",
    "    #get max token and tag length\n",
    "    n_token = len(list(set(data['Word'].to_list())))\n",
    "    n_tag = len(list(set(data['Tag'].to_list())))\n",
    "\n",
    "    #Pad tokens (X var)    \n",
    "    tokens = data_group['Word_idx'].tolist()\n",
    "    maxlen = max([len(s) for s in tokens])\n",
    "    pad_tokens = pad_sequences(tokens, maxlen=maxlen, dtype='int32', padding='post', value= n_token - 1)\n",
    "\n",
    "    #Pad Tags (y var) and convert it into one hot encoding\n",
    "    tags = data_group['Tag_idx'].tolist()\n",
    "    pad_tags = pad_sequences(tags, maxlen=maxlen, dtype='int32', padding='post', value= tag2idx[\"O\"])\n",
    "    n_tags = len(tag2idx)\n",
    "    pad_tags = [to_categorical(i, num_classes=n_tags) for i in pad_tags]\n",
    "    \n",
    "    #Split train, test and validation set\n",
    "    tokens_, test_tokens, tags_, test_tags = train_test_split(pad_tokens, pad_tags, test_size=0.1, train_size=0.9, random_state=2020)\n",
    "    train_tokens, val_tokens, train_tags, val_tags = train_test_split(tokens_,tags_,test_size = 0.25,train_size =0.75, random_state=2020)\n",
    "\n",
    "    print(\n",
    "        'train_tokens length:', len(train_tokens),\n",
    "        '\\ntrain_tokens length:', len(train_tokens),\n",
    "        '\\ntest_tokens length:', len(test_tokens),\n",
    "        '\\ntest_tags:', len(test_tags),\n",
    "        '\\nval_tokens:', len(val_tokens),\n",
    "        '\\nval_tags:', len(val_tags),\n",
    "    )\n",
    "    \n",
    "    return train_tokens, val_tokens, test_tokens, train_tags, val_tags, test_tags\n",
    "\n",
    "train_tokens, val_tokens, test_tokens, train_tags, val_tags, test_tags = get_pad_train_test_val(data_group, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201c2450",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training Neural Network for Named Entity Recognition (NER)\n",
    "import numpy as np\n",
    "import tensorflow\n",
    "from tensorflow.keras import Sequential, Model, Input\n",
    "from tensorflow.keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "tensorflow.random.set_seed(2)\n",
    "input_dim = len(list(set(data['Word'].to_list())))+1\n",
    "output_dim = 64\n",
    "input_length = max([len(s) for s in data_group['Word_idx'].tolist()])\n",
    "n_tags = len(tag2idx)\n",
    "def get_bilstm_lstm_model():\n",
    "    model = Sequential()\n",
    "\n",
    "    # Add Embedding layer\n",
    "    model.add(Embedding(input_dim=input_dim, output_dim=output_dim, input_length=input_length))\n",
    "\n",
    "    # Add bidirectional LSTM\n",
    "    model.add(Bidirectional(LSTM(units=output_dim, return_sequences=True, dropout=0.2, recurrent_dropout=0.2), merge_mode = 'concat'))\n",
    "\n",
    "    # Add LSTM\n",
    "    model.add(LSTM(units=output_dim, return_sequences=True, dropout=0.5, recurrent_dropout=0.5))\n",
    "\n",
    "    # Add timeDistributed Layer\n",
    "    model.add(TimeDistributed(Dense(n_tags, activation=\"relu\")))\n",
    "\n",
    "    #Optimiser \n",
    "    # adam = k.optimizers.Adam(lr=0.0005, beta_1=0.9, beta_2=0.999)\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64e14dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X, y, model):\n",
    "    loss = list()\n",
    "    for i in range(25):\n",
    "        # fit model for one epoch on this sequence\n",
    "        hist = model.fit(X, y, batch_size=1000, verbose=1, epochs=1, validation_split=0.2)\n",
    "        loss.append(hist.history['loss'][0])\n",
    "    return loss\n",
    "Code language: Python (python)\n",
    "#Driver code:\n",
    "results = pd.DataFrame()\n",
    "model_bilstm_lstm = get_bilstm_lstm_model()\n",
    "plot_model(model_bilstm_lstm)\n",
    "results['with_add_lstm'] = train_model(train_tokens, np.array(train_tags), model_bilstm_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3095e477",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now let’s test our model on a piece of text:\n",
    "\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "text = nlp('Hi, My name is Aman Kharwal \\n I am from India \\n I want to work with Google \\n Steve Jobs is My Inspiration')\n",
    "displacy.render(text, style = 'ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a5eb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "source:\n",
    "    https://thecleverprogrammer.com/2020/08/04/named-entity-recognition-ner/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
