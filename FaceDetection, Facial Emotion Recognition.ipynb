{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e40d7c83",
   "metadata": {},
   "source": [
    "# using openCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c36f7d5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-05T20:50:10.426760Z",
     "start_time": "2023-07-05T20:49:42.575142Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in c:\\users\\hp\\anaconda3\\lib\\site-packages (4.6.0.66)\n",
      "Requirement already satisfied: numpy>=1.19.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from opencv-python) (1.22.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -pencv-python-headless (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python-headless (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python-headless (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python-headless (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python-headless (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python-headless (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b908e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot photo with detected faces using opencv cascade classifier\n",
    "from cv2 import imread\n",
    "from cv2 import imshow\n",
    "from cv2 import waitKey\n",
    "from cv2 import destroyAllWindows\n",
    "from cv2 import CascadeClassifier\n",
    "from cv2 import rectangle\n",
    "# load the photograph\n",
    "pixels = imread( 'faces.jpg')\n",
    "# load the pre-trained model\n",
    "classifier = Cascadeclassifier('haarcascade_frontalface_default.xml')\n",
    "# perform face detection\n",
    "bboxes = classifier.detectMultiScale (pixels)\n",
    "# print bounding box for each detected face\n",
    "for box in boxes:\n",
    "    # extract\n",
    "    X, Y, width, height = box\n",
    "    X2, y2 = x + width, y + height\n",
    "    # draw a rectangle over the pixels\n",
    "    rectangle (pixels, (x, y), (x2, y2), (0,0,255), 1)\n",
    "# show the image\n",
    "imshow('face detection', pixels)\n",
    "# keep the window open until we press a key\n",
    "waitKey(0)\n",
    "# close the window\n",
    "destroyAllWindows ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f92edd8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-05T20:58:03.490861Z",
     "start_time": "2023-07-05T20:57:55.994356Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mcnn"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -pencv-python-headless (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python-headless (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python-headless (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python-headless (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python-headless (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python-headless (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python-headless (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Downloading mcnn-1.1.2-py3-none-any.whl (5.8 kB)\n",
      "Requirement already satisfied: tensorflow in c:\\users\\hp\\anaconda3\\lib\\site-packages (from mcnn) (2.11.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\hp\\anaconda3\\lib\\site-packages (from mcnn) (1.22.4)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\hp\\anaconda3\\lib\\site-packages (from mcnn) (4.6.0.66)\n",
      "Requirement already satisfied: tensorflow-intel==2.11.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow->mcnn) (2.11.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow->mcnn) (1.3.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow->mcnn) (0.28.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow->mcnn) (3.6.0)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow->mcnn) (0.4.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow->mcnn) (1.16.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow->mcnn) (22.11.23)\n",
      "Requirement already satisfied: tensorboard<2.12,>=2.11 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow->mcnn) (2.11.0)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow->mcnn) (3.19.6)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow->mcnn) (1.12.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow->mcnn) (61.2.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow->mcnn) (21.3)\n",
      "Requirement already satisfied: keras<2.12,>=2.11.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow->mcnn) (2.11.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow->mcnn) (2.11.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow->mcnn) (3.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow->mcnn) (4.1.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow->mcnn) (1.6.3)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow->mcnn) (2.1.1)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow->mcnn) (14.0.6)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow->mcnn) (0.2.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow->mcnn) (1.42.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.11.0->tensorflow->mcnn) (0.37.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow->mcnn) (2.27.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow->mcnn) (0.4.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow->mcnn) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow->mcnn) (1.8.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow->mcnn) (2.0.3)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow->mcnn) (3.3.4)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow->mcnn) (1.33.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from packaging->tensorflow-intel==2.11.0->tensorflow->mcnn) (3.0.4)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow->mcnn) (4.2.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow->mcnn) (4.7.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow->mcnn) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow->mcnn) (1.3.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow->mcnn) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow->mcnn) (2.10)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow->mcnn) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow->mcnn) (2021.10.8)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow->mcnn) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow->mcnn) (3.2.2)\n",
      "Installing collected packages: mcnn\n",
      "Successfully installed mcnn-1.1.2\n"
     ]
    }
   ],
   "source": [
    "!pip install mcnn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efdf0dc9",
   "metadata": {},
   "source": [
    "# Using Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934419f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# face detection with mcnn on a photograph\n",
    "from matplotlib import pyplot\n",
    "from mtcnn.mcnn import MTCNN\n",
    "# Load image from file\n",
    "filename = 'faces.jpg'\n",
    "pixels = pyplot.imread(filename)\n",
    "# create the detector, using default weights\n",
    "detector = MTCNN ()\n",
    "# detect faces in the image\n",
    "faces = detector. detect_faces (pixels)\n",
    "for face in faces:\n",
    "    print(face)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d0d4b9",
   "metadata": {},
   "source": [
    "# Using Deep Learning - Draw Boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76da0419",
   "metadata": {},
   "outputs": [],
   "source": [
    "# face detection with mtenn on a photograph\n",
    "from matplotlib import pyplot\n",
    "from matplotlib.patches import Rectangle\n",
    "from mcnn.mtenn import MTCNN\n",
    "# draw an image with detected objects\n",
    "def draw_image_with_boxes (filename, result_list):\n",
    "    # load the image\n",
    "    data = pyplot.imread(filename)\n",
    "    # plot the image\n",
    "    pyplot.imshow(data)\n",
    "    # get the context for drawing boxes\n",
    "    ax = pyplot.gca()\n",
    "    # plot each\n",
    "    for result in result list:\n",
    "        # get coordinates\n",
    "        X, Y, width, height = result ['box']\n",
    "        # create the shape\n",
    "        rect = Rectangle((x, y), width, height, fill=False, color='red')\n",
    "        # draw the box\n",
    "        ax.add_patch(rect)\n",
    "    # show the plot\n",
    "    pyplot.show()\n",
    "filename = 'facese. jpg'\n",
    "# Load image from file\n",
    "pixels = pyplot.imread(filename)\n",
    "# create the detector, using default weights\n",
    "detector = MTCNN()\n",
    "# detect faces in the image\n",
    "faces = detector. detect_faces (pixels)\n",
    "# display faces on the original image\n",
    "draw_image_with_boxes(filename, faces)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a266d7db",
   "metadata": {},
   "source": [
    "# Using Deep Learning - Draw a circle for the eyes, nose, and mouth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498f2e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from mcnn.mtenn import MCNN\n",
    "detector = MTCNN( )\n",
    "image = cv2.imread(\"happyl.jpg\")\n",
    "result = detector.detect_faces (image)\n",
    "bounding_box = result[0][ 'box']\n",
    "keypoints = result [0]['keypoints']\n",
    "cv2. rectangle (image,(bounding_box[0], bounding_box[1]),(bounding_box[0]+bounding_box[2],bounding_box[1] + bounding_box [3]), (0,155,255), 2)\n",
    "cv2. circle(image, (keypoints ['left_eye']), 2, (0, 155,255), 2)\n",
    "cv2.circle(image, (keypoints['right_eye']), 2, (0,155, 255), 2)\n",
    "cv2.circle (image, (keypoints['nose']), 2, (0,155,255), 2)\n",
    "cv2.circle(image, (keypoints['mouth_left']), 2, (0,155,255), 2)\n",
    "cv2. circle(image, (keypoints ['mouth_right']), 2, (0,155,255), 2)\n",
    "cv2.imwrite(\"ivan_drawn.jpg\", image)\n",
    "cv2.namedWindow(\"image\")\n",
    "cv2.imshow(\"image\" ,image)\n",
    "cv2.waitKey(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dbcdad5",
   "metadata": {},
   "source": [
    "# Using Deep Learning - Draw a circle for the eyes, nose, and mouth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1ee2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "from matplotlib.patches import Rectangle\n",
    "from matplotlib.patches import Circle\n",
    "from mtenn.mtcnn import MTCNN\n",
    "# draw an image with detected objects\n",
    "def draw_image_with_boxes(filename, result_list):\n",
    "    # Load the image\n",
    "    data = byplot. imread (filename)\n",
    "    # plot\n",
    "    pyplot.imshow(data)\n",
    "    # get the context for drawing boxes\n",
    "    ax = pyplot.gca()\n",
    "    # bLot each box\n",
    "    for result in result_list:\n",
    "        # get coordinates\n",
    "        x, y, width, height = result[ 'box']\n",
    "        # create the shave\n",
    "        rect = Rectangle((x, y), width, height, fill=False, color='red')\n",
    "        # draw the box\n",
    "        ax.add_patch(rect)\n",
    "        # draw the dots\n",
    "        for key, value in result ['keypoints'].items():\n",
    "            # create and draw dot\n",
    "            dot = Circle (value, radius=2, color='red')\n",
    "\n",
    "            ax.add_patch(dot)\n",
    "    # show the plot\n",
    "    pyplot.show()\n",
    "filename = 'faces2. jpg'\n",
    "# Load image from file\n",
    "pixels = pyplot.imread(filename)\n",
    "# create the detector, using default weights\n",
    "detector = MTCNN()\n",
    "# detect faces in the image\n",
    "faces = detector. detect_faces (pixels)\n",
    "# display faces on the original image\n",
    "draw_image_with_boxes (filename, faces)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37aae795",
   "metadata": {},
   "source": [
    "# Extract a face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38a4125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of face detection with mcnn\n",
    "from matplotlib import pyplot\n",
    "from PIL import Image\n",
    "from numpy import asarray\n",
    "from mtcnn.mcnn impot MTCNN\n",
    "# extract a single face from a given photograph\n",
    "def extract_face(filename, required_size= (224, 224)):\n",
    "    # Load image from file\n",
    "    pixels = pyplot.imread(filename)\n",
    "    # create the detector, using default weights\n",
    "    detector = MTCNN()\n",
    "    # detect faces in the image\n",
    "    results = detector. detect_faces (pixels)\n",
    "    # extract the bounding box from the first face\n",
    "    x1, y1, width, height = results[0]['box']\n",
    "    X2, y2 = x1 + width, y1 + height\n",
    "    # extract the face\n",
    "    face = pixels [y1:y2, ×1:×2]\n",
    "    # resize pixels to the model size\n",
    "    image = Image.fromarray (face)\n",
    "    image = image.resize (required_size)\n",
    "    face_array = asarray (image)\n",
    "    return face_array\n",
    "# Load the photo and extract the face\n",
    "pixels = extract face('faces1.jpg')'\n",
    "# plot the extracted face\n",
    "pyplot.imshow(pixels)\n",
    "# show the plot\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4652d3",
   "metadata": {},
   "source": [
    "# Using Deen Learning - Extract Faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7494e18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract and plot each detected face in a photograph\n",
    "from matplotlib import pyplot\n",
    "from matplotlib.patches import Rectangle\n",
    "from matplotlib.patches import Circle\n",
    "from mcnn.mcnn import MCNN\n",
    "# draw each face separately\n",
    "def draw_faces (filename, result_list):\n",
    "    # load the image\n",
    "    data = pyplot.imread (filename)\n",
    "    # plot each face as a subplot\n",
    "    for i in range(len (result_list)):\n",
    "        # get coordinates\n",
    "        x1, y1, width, height = result list[i]['box']\n",
    "        X2, y2 = x1 + width, y1 + height\n",
    "        # define subplot\n",
    "        pyplot. subplot(1, len(result_list), i+1) pyplot.axis('off')\n",
    "        # plot face\n",
    "        pyplot.imshow(data[y1:2, x1:×2])\n",
    "    # show the plot\n",
    "    pyplot. show()\n",
    "filename = 'facese.jpg'\n",
    "# load image from file\n",
    "pixels = pyplot.imread(filename)\n",
    "# create the detector, using default weights\n",
    "detector = MTCNN()\n",
    "# detect faces in the image\n",
    "faces = detector.detect_faces (pixels)\n",
    "# display faces on the original image\n",
    "draw faces (filename, faces)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e317e8",
   "metadata": {},
   "source": [
    "# VideoCapture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9218e514",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from mcnn.mtenn import MCNN\n",
    "detector = MTCNN()\n",
    "cap = cv2. VideoCapture(0)\n",
    "while True:\n",
    "    #Capture frame-by-frame\n",
    "    __.frame = cap.read ()\n",
    "    #Use MTCNN to detect faces\n",
    "    result = detector.detect_faces (frame)\n",
    "    if result != []:\n",
    "        for person in result:\n",
    "            bounding_box = person [ 'box']\n",
    "            keypoints = person ['keypoints']\n",
    "            cv2. rectangle(frame,(bounding_box[0], bounding_box[1]), (bounding_box[0]+bounding_box [2].bounding_box[1] + bounding_box[3]), (0, 155,255),2)\n",
    "            cv2.circle(frame, (keypoints ['left_eye']), 2, (0,155,255), 2)\n",
    "            cv2.circle(frame, (keypoints['right_eye']), 2, (0,155,255), 2)\n",
    "            cv2.circle(frame, (keypoints ['nose']), 2, (0,155,255), 2) \n",
    "            cv2. circle (frame, (keypoints ['mouth_left']), 2, (0,155,255), 2)\n",
    "            cv2. circle (frame, (keypoints ['mouth_right'1]), 2, (0,155,255), 2) \n",
    "    #display resulting frame\n",
    "    cv2.imshow('frame',frame)\n",
    "    if cv2.waitKey(1) &0xFF == ord('q'):\n",
    "        break\n",
    "#When everything's done, release capture\n",
    "cap.release ()\n",
    "cv2. destroyAllWindows ()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54983cb",
   "metadata": {},
   "source": [
    "# Facial Emotion Recognition\n",
    "\n",
    "EMOTIONS = [\n",
    "\"angry\",\n",
    "\"disgust\",\n",
    "\"scared\",\n",
    "\"happy\",\n",
    "\"sad\",\n",
    "\"surprised\",\n",
    "\"neutral\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4e1356",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.models import load_model\n",
    "import cv2\n",
    "import numpy as np\n",
    "import sys\n",
    "# parameters for loading data and images\n",
    "detection_model_path = 'haarcascade_frontalface_default.xmi'\n",
    "emotion_model_path =\n",
    "img_path = 'happy1. jpg\n",
    "# loading models\n",
    "face_detection = cv2. CascadeClassifier (detection_model_path)\n",
    "emotion_classifier = load model(emotion_model_path,compile=False)\n",
    "EMOTIONS = [\"angry\",\"disgust\", \"scared\", \"happy\",\"sad\",\"surprised\", \"neutral\"]\n",
    "#reading the frame\n",
    "orig_frame = cv2. imread (img_path)\n",
    "frame = cv2.imread(img_path,o)\n",
    "faces = face_detection. detectMultiScale (frame, scaleFactor=1.1, minNeighbors=5,minsize=(30,30), \\\n",
    "                                          flags=cv2.CASCADE_SCALE_IMAGE)\n",
    "if len (faces) > 0:\n",
    "    faces = sorted(faces, reverse=True, key=lambda x: (x[2] - x[0]) * (x[3] - x[1])) [0]\n",
    "    (fX, fY, fW, fH) = faces\n",
    "    roi = frame [fY:fY + fH, fX: fX + fW]\n",
    "    roi = cv2.resize(roi, (48, 48))\n",
    "    roi = roi.astype(\"float\") / 255.0\n",
    "    roi = img_to_array(roi)\n",
    "    roi = np.expand_dims (roi, axis=0)\n",
    "    preds = emotion classifier.predict(roi)[0]\n",
    "    emotion_probability = np. max(preds)\n",
    "    label = EMOTIONS [preds. argmax ()]\n",
    "    cv2.putText(orig_frame, label, (fX, fy - 10), cv2. FONT_HERSHEY_SIMPLEX, 0.45, (0, 0, 255), 2)\n",
    "    cv2.rectangle(orig_frame, (fX, fY), (fX + fW, fY + fH), (0, 0, 255), 2)\n",
    "cv2.imshow('test_face', orig_frame)\n",
    "cv2.imwrite ('test_output/'+img_path.split('/')[-1],orig_frame)\n",
    "if (cv2.waitKey (2000) & 0xFF == ord('q')):\n",
    "    sys.exit(\"Thanks\")\n",
    "cv2. destroyAllWindows ()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081d9d11",
   "metadata": {},
   "source": [
    "# Facial Emotion Recognition - Video/ Camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea86849e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import img_to_array\n",
    "import imutils\n",
    "import cv2\n",
    "from keras. models import load_model\n",
    "import numpy as np\n",
    "# parameters for loading data and images\n",
    "detection_model_path = 'haarcascade_frontalface_default.xml'\n",
    "emotion_model_path = '_mini_XCEPTION.106-0.65.hdf5'\n",
    "# hyper-parameters for bounding boxes shape\n",
    "# loading models\n",
    "face_detection = cv2. CascadeClassifier (detection _model_path)\n",
    "emotion_classifier = load model(emotion model_path, compile=False)\n",
    "EMOTIONS = [\"angry\" ,\"disgust\",\"scared\", \"happy\",\"sad\", \"surprised\", \"neutral\"]\n",
    "# starting video streaming\n",
    "cv2.namedWindow('your_face')\n",
    "camera = cv2. VideoCapture (0)\n",
    "while True:\n",
    "    frame = camera.read () [1]\n",
    "    #reading the frame\n",
    "    frame = imutils.resize(frame,width=400)\n",
    "    gray = cv2. cvtColor (frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_detection.detectMultiScale (gray, scaleFactor=1.1, minNeighbors=5,\\\n",
    "                                             minsize= (30,30), flags=cv2.CASCADE_SCALE_IMAGE)\n",
    "    canvas = np.zeros ( (250, 300, 3), dtype=\"uint8\")\n",
    "    frameClone = frame. copy ()\n",
    "    if len(faces) > 0:          \n",
    "        faces = sorted(faces, reverse=True,key=lambda x: (x[2] - x[0]) * (x[3] - x[1]))[0]\n",
    "        (fX,fY,tW, tH) = faces\n",
    "        roi = gray[fY: fY + fH, fX: fX + fW]\n",
    "        roi = cv2.resize (roi, (48, 48))\n",
    "        roi = roi.astype(\"float\") / 255.0\n",
    "        roi = img_to_array (roi)\n",
    "        roi = np.expand_dims (roi, axis=0)\n",
    "        preds = emotion classifier.predict(roi)[0]\n",
    "        emotion_probability = np.max (preds)\n",
    "        label = EMOTIONS [preds.argmax ()]\n",
    "    for (i, (emotion, prob)) in enumerate (zip(EMOTIONS, preds)):\n",
    "        # construct the label text\n",
    "        text = \"{}: {:.2f}%\" .format (emotion, prob * 100)\n",
    "        w = int (prob * 300)\n",
    "        cv2.rectangle (canvas, (7, (i * 35) + 5), (w, (i * 35) + 35), (0, 0, 255), -1)\n",
    "        cv2.putText (canvas, text, (10, (i * 35) + 23),\n",
    "        cV2. FONT_HERSHEY_SIMPLEX, 0.45,(255, 255, 255), 2)\n",
    "        cv2.putText (frameClone, label, (fX, fY - 10),\n",
    "        cV2. FONT_HERSHEY_SIMPLEX, 0.45, (0, 0, 255), 2)\n",
    "        cv2. rectangle(frameClone, (fX, fY), (fX + fW, fY + fH),(0, 0, 255),2)\n",
    "    cv2.imshow('your_face', frameClone)\n",
    "    cv2.imshow(\"Probabilities\", canvas)\n",
    "    if cv2.waitKey (1) & 0xFF == ord('q'):\n",
    "        break\n",
    "camera.release ()\n",
    "cv2.destroyAllWindows ()        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88644a0f",
   "metadata": {},
   "source": [
    "# Face Identification With VGGFace2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d65cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install keras_vggface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be35f816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of face detection with a vggface2 model\n",
    "from numpy import expand dims\n",
    "from matplotlib import pyplot\n",
    "from PIL import Image\n",
    "from numpy import asarray\n",
    "from mtcnn.mcnn import MTCNN\n",
    "from keras_vggface.vggface import VGGFace\n",
    "from keras_vggface.utils import preprocess input\n",
    "from keras_vggface.utils import decode_predictions\n",
    "# extract a single face from a given photograph\n",
    "def extract_face(filename, required_size= (224, 224)):\n",
    "    # Load image from file\n",
    "    pixels = pyplot.imread(filename)\n",
    "    # create the detector, using default weights\n",
    "    detector = MTCNN()\n",
    "    # detect faces in the image\n",
    "    results = detector.detect_faces (pixels)\n",
    "    # extract the bounding box from the first face\n",
    "    x1, y1, width, height = results [0]['box']\n",
    "    X2, y2 = x1 + width, y1 + height\n",
    "    # extract the face\n",
    "    face = pixels [y1:2, x1:×2]\n",
    "    # resize pixels to the model size\n",
    "    image = Image.fromarray (face)\n",
    "    image = image.resize(required_size)\n",
    "    face_array = asarray (image)\n",
    "    return face_array\n",
    "# Load the photo and extract the face\n",
    "pixels = extract_face ('sharon_stonel.jpg')\n",
    "# convert one face into samples\n",
    "pixels = pixels.astype ('float32')\n",
    "samples = expand_dims (pixels, axis=0)\n",
    "# prepare the face for the model, e.g. center pixels\n",
    "samples = preprocess_input (samples, version=2)\n",
    "# create a vggface model\n",
    "model = VGGFace (model='resnet50')\n",
    "# perform prediction\n",
    "yhat = model.predict(samples)\n",
    "# convert prediction into names\n",
    "results = decode predictions (yhat)\n",
    "# display most likely results\n",
    "for result in results [0]:\n",
    "    print('%s: %.3f%%' % (result[e], result[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c2dc2e",
   "metadata": {},
   "source": [
    "# Face Verification With VGGFace2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bc5cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#face verrrrcation with the VeGFace2 mode\n",
    "from matplotlib import pyplot\n",
    "from PIL import Image\n",
    "from numpy import asarray\n",
    "from scipy.spatial.distance import cosine\n",
    "from mtcnn.mtenn import MCNN\n",
    "from keras_vggface.vggface import VGGFace\n",
    "from keras_vggface.utils import preprocess_input\n",
    "# extract a single face from a given photograph\n",
    "def extract_face (filename, required_size= (224, 224)):\n",
    "   # Load image from file \n",
    "    pixels = pyplot.imread(filename)\n",
    "    # create the detector, using default weights\n",
    "    detector = MTCNN()\n",
    "    # detect faces in the image\n",
    "    results = detector.detect_faces (pixels)\n",
    "    # extract the bounding box from the first face\n",
    "    ×1, y1, width, height = results [0]['box']\n",
    "    ×2, y2 = x1 + width, y1 + height\n",
    "    # extract the face\n",
    "    face = pixels [y1:y2, x1:×2]\n",
    "    # resize pixels to the model size\n",
    "    image = Image.fromarray (face)\n",
    "    image = image.resize (required_size)\n",
    "    face_array = asarray (image)\n",
    "    return face_array\n",
    "# extract faces and calculate face embeddings \n",
    "def get_embeddings (filenames) :\n",
    "    # extract faces\n",
    "    faces = [extract face(f) for f in filenames]\n",
    "    # convert into an array of samples\n",
    "    samples = asarray (faces, 'float32')\n",
    "    # prepare the face for the model, e.g. center pixels\n",
    "    samples = preprocess_input (samples, version=2)\n",
    "    # create a vaaface model\n",
    "    model = VGGFace (model= 'resnet50', include_top=False,input_shape=(224, 224, 3), pooling='avg')\n",
    "    # perform prediction\n",
    "    yhat = model.predict(samples)\n",
    "    return yhat\n",
    "# determine if a candidate face is a match for a known face\n",
    "def is_match (known_embedding, candidate_embedding, thresh=0.5):\n",
    "    # calculate distance between embeddings\n",
    "    score = cosine (known_embedding, candidate_embedding)\n",
    "    if score <= thresh:\n",
    "        print('>face is a Match (%.3f <= %.3f) ' % (score, thresh))\n",
    "    else:\n",
    "        print ('>face is NOT a Match (%.3f > %.3f) ' % (score, thresh))\n",
    "# define filenames\n",
    "filenames = ['sharon_stone1.jpg', 'sharon_stone2.jpg','sharon_stone3.jpg','channing_tatum.jpg']\n",
    "# get embeddings file filenames\n",
    "embeddings = get_embeddings (filenames)\n",
    "# define sharon stone\n",
    "sharon_id = embeddings [0]\n",
    "# verify known photos of sharon\n",
    "print('Positive Tests')\n",
    "is_match(embeddings [0], embeddings [1])\n",
    "is_match(embeddings [0], embeddings [2])\n",
    "# verify known photos of other people\n",
    "print('Negative Tests')\n",
    "is_match(embeddings [0], embeddings [3])\n",
    "# show percent of face is match or not and the test is positive or negative"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c26c3a",
   "metadata": {},
   "source": [
    "# References:\n",
    "\n",
    "https://raw.githubusercontent.com/opencv/opencv/master/data/haarcascades/haarcascade_frontalface_default.xml\n",
    "\n",
    "https://machinelearningmastery.com/how-to-perform-face-detection-with-classical-and-deep-learning-methods-in-python-with-keras/\n",
    "\n",
    "https://machinelearningmastery.com/how-to-perform-face-recognition-\n",
    "\n",
    "https://towardsdatascience.com/mtcnn-face-detection-cdcb20448ce0\n",
    "    \n",
    "https://machinelearningmastery.com/how-to-perform-face-recognition-with-vggface2-convolutional-neural-network-in-keras\n",
    "\n",
    "https://github.com/ipazc/mtcnn/blob/master/example.py\n",
    "\n",
    "https://github.com/abhijeet3922/FaceEmotion\n",
    "\n",
    "https://github.com/abhijeet3922/FaceEmotion_ID/tree/master/models\n",
    "\n",
    "https://github.com/abhijeet3922/FaceEmotion_ID/tree/master/haarcascade_files\n",
    "\n",
    "https://appliedmachinelearning.blog/2018/11/28/demonstration-of-facial-emotion-recognition-on-real-time-video-using-cnn-python-keras/\n",
    "\n",
    "https://github.com/abhijeet3922/FaceEmotion_ID/blob/master/facial_emotion_image.py\n",
    "\n",
    "https://github.com/abhijeet3922/FaceEmotion_ID/blob/master/real_time_video.py\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1196bb50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
